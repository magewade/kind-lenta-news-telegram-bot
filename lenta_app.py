import streamlit as st
import pandas as pd
import sqlite3
from parsing import run_parsing
from datetime import datetime, timedelta
from sentence_transformers import SentenceTransformer
import matplotlib.pyplot as plt
from database.news_db import search_news_by_keyword
from wordcloud import WordCloud
import seaborn as sns
from matplotlib.colors import LinearSegmentedColormap
import numpy as np
from model.nltk_stopwords import russian_stopwords
from pymorphy2 import MorphAnalyzer
import re

morph = MorphAnalyzer()

def lemmatize_text(text):
    words = re.findall(r"\b[–∞-—è–ê-–Ø—ë–Å]{3,}\b", text.lower())
    lemmas = [
        morph.parse(word)[0].normal_form
        for word in words
        if word not in russian_stopwords
    ]
    return " ".join(lemmas)

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
def load_articles_from_db(path_to_db="news_database.db"):
    conn = sqlite3.connect(path_to_db)
    df = pd.read_sql("SELECT * FROM articles", conn)
    conn.close()
    df["normalized_date"] = pd.to_datetime(df["normalized_date"], errors="coerce")
    return df

st.set_page_config(page_title="–ù–æ–≤–æ—Å—Ç–∏", layout="wide")
st.title("üóûÔ∏è –î–æ–±—Ä—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å —Å–∞–π—Ç–∞ Lenta.ru")

@st.cache_resource
def load_model():
    return SentenceTransformer("sberbank-ai/sbert_large_nlu_ru")

model = load_model()

# # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞
# if st.button("üîÉ –û–±–Ω–æ–≤–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏"):
#     with st.spinner("–ü–∞—Ä—Å–∏–º –Ω–æ–≤–æ—Å—Ç–∏..."):
#         run_parsing()
#     st.success("‚úÖ –ù–æ–≤–æ—Å—Ç–∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã!")
# –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –∑–∞–¥–µ—Ä–∂–∫–µ –≤ –ø–∞—Ä—Å–∏–Ω–≥–µ
st.info(
    "–≠—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã, –∏ –Ω–æ–≤–æ—Å—Ç–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –æ—Ç–ø–∞—Ä—Å–µ–Ω—ã —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π –≤–æ –≤—Ä–µ–º–µ–Ω–∏. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∏–º–µ–π—Ç–µ –≤ –≤–∏–¥—É, —á—Ç–æ –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ '—Å–µ–≥–æ–¥–Ω—è' –∏–ª–∏ '–≤—á–µ—Ä–∞' –±—É–¥—É—Ç –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å—Å—è –æ—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ –¥–Ω—è."
)

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
df = load_articles_from_db()

# –í–ö–õ–ê–î–ö–ò
tab1, tab2, tab3 = st.tabs(["üîç –ù–æ–≤–æ—Å—Ç–∏ –ø–æ —Ç–µ–º–µ", "üóìÔ∏è –ù–æ–≤–æ—Å—Ç–∏ –ø–æ –¥–∞—Ç–µ", "üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"])

# --- –¢–ê–ë 1: –ü–æ–∏—Å–∫ –ø–æ —Ç–µ–º–µ ---
with tab1:
    st.header("üîé –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ç–µ–º–µ")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤ session_state
    if "search_clicked" not in st.session_state:
        st.session_state.search_clicked = False
    if "last_keyword" not in st.session_state:
        st.session_state.last_keyword = ""

    # –í–≤–æ–¥ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
    keyword = st.text_input("–í–≤–µ–¥–∏—Ç–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ —Ñ—Ä–∞–∑—É –¥–ª—è –ø–æ–∏—Å–∫–∞:")

    # –û–±–Ω—É–ª—è–µ–º —Ñ–ª–∞–≥ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞
    if keyword != st.session_state.last_keyword:
        st.session_state.search_clicked = False
        st.session_state.last_keyword = keyword

    # –ö–Ω–æ–ø–∫–∞ –∑–∞–ø—É—Å–∫–∞ –ø–æ–∏—Å–∫–∞
    if st.button("üîç –ò—Å–∫–∞—Ç—å"):
        st.session_state.search_clicked = True

    # –ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞ (threshold)
    similarity_threshold = st.slider(
        "–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è", 0.0, 1.0, 0.25, 0.05
    )

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ
    sort_by_date = st.selectbox(
        "–°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–∞—Ç–µ:", ["–û—Ç –Ω–æ–≤—ã—Ö –∫ —Å—Ç–∞—Ä—ã–º", "–û—Ç —Å—Ç–∞—Ä—ã—Ö –∫ –Ω–æ–≤—ã–º"]
    )

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏
    sort_by_similarity = st.selectbox(
        "–°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏:",
        ["–û—Ç –±–æ–ª—å—à–µ–≥–æ –∫ –º–µ–Ω—å—à–µ–º—É", "–û—Ç –º–µ–Ω—å—à–µ–≥–æ –∫ –±–æ–ª—å—à–µ–º—É"],
    )

    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
    if st.session_state.search_clicked and keyword:
        with st.spinner("–ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏..."):
            try:
                results = search_news_by_keyword(keyword)

                if not results:
                    st.info("–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –ø–æ—Ö–æ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
                else:
                    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø–æ—Ä–æ–≥—É —Å—Ö–æ–¥—Å—Ç–≤–∞
                    results = [
                        res
                        for res in results
                        if res["similarity"] >= similarity_threshold
                    ]

                    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ
                    if sort_by_date == "–û—Ç –Ω–æ–≤—ã—Ö –∫ —Å—Ç–∞—Ä—ã–º":
                        results = sorted(results, key=lambda x: x["date"], reverse=True)
                    else:
                        results = sorted(results, key=lambda x: x["date"])

                    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏
                    if sort_by_similarity == "–û—Ç –±–æ–ª—å—à–µ–≥–æ –∫ –º–µ–Ω—å—à–µ–º—É":
                        results = sorted(
                            results, key=lambda x: x["similarity"], reverse=True
                        )
                    else:
                        results = sorted(results, key=lambda x: x["similarity"])

                    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    for res in results:
                        st.markdown(f"### [{res['title']}]({res['url']})")
                        st.write(f"{res['content'][:3000]}...")
                        st.write(f"–î–∞—Ç–∞: {res['date']}")
                        st.write(f"–°—Ö–æ–¥—Å—Ç–≤–æ: {res['similarity']:.2f}")
                        st.write("---")

            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")


# --- –¢–ê–ë 2: –ù–æ–≤–æ—Å—Ç–∏ –ø–æ –¥–∞—Ç–µ ---
with tab2:
    st.header("üóìÔ∏è –§–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ")
    

    # –ü–æ–ª—É—á–∞–µ–º —Å–∞–º—É—é –ø–æ—Å–ª–µ–¥–Ω—é—é –¥–∞—Ç—É –≤ –±–∞–∑–µ
    last_date = df["normalized_date"].max().date()

    date_filter = st.selectbox(
        "–í—ã–±–µ—Ä–∏—Ç–µ –ø–µ—Ä–∏–æ–¥:", ("–°–µ–≥–æ–¥–Ω—è", "–í—á–µ—Ä–∞", "–ë–æ–ª–µ–µ —Ä–∞–Ω–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏")
    )

    def filter_news_by_date(date_filter):
        if date_filter == "–°–µ–≥–æ–¥–Ω—è":
            filtered_df = df[df["normalized_date"].dt.date == last_date]
        elif date_filter == "–í—á–µ—Ä–∞":
            yesterday = last_date - timedelta(days=1)
            filtered_df = df[df["normalized_date"].dt.date == yesterday]
        else:
            filtered_df = df[
                df["normalized_date"].dt.date
                < (last_date - timedelta(days=1)).date()
            ]
        return filtered_df.sort_values(by="normalized_date", ascending=False)

    filtered_news = filter_news_by_date(date_filter)

    if filtered_news.empty:
        st.write(f"–ù–æ–≤–æ—Å—Ç–∏ –∑–∞ {date_filter.lower()} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
    else:
        for _, news_item in filtered_news.iterrows():
            st.markdown(f"### [{news_item['title']}]({news_item['url']})")
            st.write(f"{news_item['content'][:3000]}...")
            st.write(
                f"–î–∞—Ç–∞: {news_item['normalized_date'].strftime('%d.%m.%Y, %H:%M')}"
            )
            st.write(f"–ê–≤—Ç–æ—Ä: {news_item['author']}")
            st.write("---")


# --- –¢–ê–ë 3: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ---
with tab3:
    st.header("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π")
    st.info(
        "–î–ª—è –ø–æ–¥–≥—Ä—É–∑–∫–∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤, —Ç–∏–ø–∞ –æ–±–ª–∞–∫–∞ —Å–ª–æ–≤, —Ç—Ä–µ–±—É–µ—Ç—Å—è –≤—Ä–µ–º—è. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ."
    )

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–∞—à–∏—Ö —Ü–≤–µ—Ç–æ–≤
    colors = ["#D28782", "#EBC678", "#B4C6D0", "#1B78AF"]

    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Ä—Ç—ã —Ü–≤–µ—Ç–æ–≤
    cmap = LinearSegmentedColormap.from_list("custom_cmap", colors, N=256)

    # –ì—Ä–∞—Ñ–∏–∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –¥–Ω—è–º
    df["date_only"] = df["normalized_date"].dt.date
    news_per_day = df.groupby("date_only").size().reset_index(name="count")

    fig, ax = plt.subplots(figsize=(15, 7))
    ax.plot(
        news_per_day["date_only"],
        news_per_day["count"],
        marker="o",
        color=colors[2],  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π —Ü–≤–µ—Ç –¥–ª—è –ª–∏–Ω–∏–∏
        markersize=6,
    )
    ax.set_title(
        "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –¥–Ω—è–º", fontsize=16, color="black"
    )  # –ß–µ—Ä–Ω—ã–π —Ü–≤–µ—Ç –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
    ax.set_xlabel("–î–∞—Ç–∞", fontsize=14)
    ax.set_ylabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ", fontsize=14)
    plt.xticks(rotation=45)
    st.pyplot(fig, use_container_width=False)

    # –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–µ–≥–∞–º (–≥–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º)
    if "tag" in df.columns:
        st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ç–µ–≥–∞–º")
        tag_counts = df["tag"].value_counts()
        fig_tags, ax_tags = plt.subplots(figsize=(15, 7))
        ax_tags.bar(
            tag_counts.index,
            tag_counts.values,
            color=cmap(
                np.linspace(0, 1, len(tag_counts))
            ),  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º cmap –≤ —Å–ø–∏—Å–æ–∫ —Ü–≤–µ—Ç–æ–≤
        )
        ax_tags.set_title(
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ç–µ–≥–∞–º", fontsize=16, color="black"
        )  # –ß–µ—Ä–Ω—ã–π —Ü–≤–µ—Ç –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
        ax_tags.set_xlabel("–¢–µ–≥", fontsize=14)
        ax_tags.set_ylabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ", fontsize=14)
        plt.xticks(rotation=90)
        st.pyplot(fig_tags, use_container_width=False)

    # –û–±–ª–∞–∫–æ —Å–ª–æ–≤ –ø–æ –≤—Å–µ–º –Ω–æ–≤–æ—Å—Ç—è–º –±–µ–∑ —Å—Ç–æ–ø-—Å–ª–æ–≤
    all_texts = " ".join(df["content"].dropna())
    lemmatized_text = lemmatize_text(all_texts)
    wordcloud = WordCloud(
        width=1500,
        height=700,
        background_color="white",
        colormap=cmap,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—à –∫–∞—Å—Ç–æ–º–Ω—ã–π colormap
        max_words=200,
        stopwords=russian_stopwords,  # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    ).generate(lemmatized_text)

    st.subheader("–û–±–ª–∞–∫–æ —Å–ª–æ–≤ –ø–æ –≤—Å–µ–º –Ω–æ–≤–æ—Å—Ç—è–º")
    st.image(wordcloud.to_array(), use_container_width=False)

    # –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–µ–º–∞–º (–≥–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º)
    if "topic" in df.columns:
        st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ç–µ–º–∞–º")
        topic_counts = df["topic"].value_counts()
        fig_topics, ax_topics = plt.subplots(figsize=(10, 6))
        ax_topics.bar(
            topic_counts.index,
            topic_counts.values,
            color=cmap(
                np.linspace(0, 1, len(topic_counts))
            ),  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º cmap –≤ —Å–ø–∏—Å–æ–∫ —Ü–≤–µ—Ç–æ–≤
        )
        ax_topics.set_title(
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ç–µ–º–∞–º", fontsize=16, color="black"
        )  # –ß–µ—Ä–Ω—ã–π —Ü–≤–µ—Ç –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
        ax_topics.set_xlabel("–¢–µ–º–∞", fontsize=14)
        ax_topics.set_ylabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ", fontsize=14)
        plt.xticks(rotation=90)
        st.pyplot(fig_topics, use_container_width=False)
